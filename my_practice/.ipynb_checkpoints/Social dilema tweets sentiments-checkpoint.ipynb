{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677dced4-c998-4725-ad0c-f4882fa0549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dheeraj/miniforge3/envs/ml/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /Users/dheeraj/miniforge3/envs/ml/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "import csv,random,pickle,os,zipfile,string,keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "#########################################################\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "###################################################################\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#########################################################\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import TextVectorization\n",
    "#########################################################\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "#########################################################\n",
    "from scipy.stats import linregress\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c42c02-eb4e-4066-a030-08e07bca5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cList = {\n",
    "    \"i`m\": \"i am\",\n",
    "    \"you`re\": \"you are\",\n",
    "    \"it`s\": \"it is\",\n",
    "    \"we`re\": \"we are\",\n",
    "    \"we`ll\": \"we will\",\n",
    "    \"That`s\": \"that is\",\n",
    "    \"haven`t\": \"have not\",\n",
    "    \"let`s\": \"let us\",\n",
    "    \"ain`t\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren`t\": \"are not / am not\",\n",
    "    \"can`t\": \"cannot\",\n",
    "    \"can`t`ve\": \"cannot have\",\n",
    "    \"`cause\": \"because\",\n",
    "    \"could`ve\": \"could have\",\n",
    "    \"couldn`t\": \"could not\",\n",
    "    \"couldn`t`ve\": \"could not have\",\n",
    "    \"didn`t\": \"did not\",\n",
    "    \"doesn`t\": \"does not\",\n",
    "    \"don`t\": \"do not\",\n",
    "    \"hadn`t\": \"had not\",\n",
    "    \"hadn`t`ve\": \"had not have\",\n",
    "    \"hasn`t\": \"has not\",\n",
    "    \"haven`t\": \"have not\",\n",
    "    \"he`d\": \"he had / he would\",\n",
    "    \"he`d`ve\": \"he would have\",\n",
    "    \"he`ll\": \"he shall / he will\",\n",
    "    \"he`ll`ve\": \"he shall have / he will have\",\n",
    "    \"he`s\": \"he has / he is\",\n",
    "    \"how`d\": \"how did\",\n",
    "    \"how`d`y\": \"how do you\",\n",
    "    \"how`ll\": \"how will\",\n",
    "    \"how`s\": \"how has / how is / how does\",\n",
    "    \"I`d\": \"I had / I would\",\n",
    "    \"I`d`ve\": \"I would have\",\n",
    "    \"I`ll\": \"I shall / I will\",\n",
    "    \"I`ll`ve\": \"I shall have / I will have\",\n",
    "    \"I`m\": \"I am\",\n",
    "    \"I`ve\": \"I have\",\n",
    "    \"isn`t\": \"is not\",\n",
    "    \"it`d\": \"it had / it would\",\n",
    "    \"it`d`ve\": \"it would have\",\n",
    "    \"it`ll\": \"it shall / it will\",\n",
    "    \"it`ll`ve\": \"it shall have / it will have\",\n",
    "    \"it`s\": \"it has / it is\",\n",
    "    \"let`s\": \"let us\",\n",
    "    \"ma`am\": \"madam\",\n",
    "    \"mayn`t\": \"may not\",\n",
    "    \"might`ve\": \"might have\",\n",
    "    \"mightn`t\": \"might not\",\n",
    "    \"mightn`t`ve\": \"might not have\",\n",
    "    \"must`ve\": \"must have\",\n",
    "    \"mustn`t\": \"must not\",\n",
    "    \"mustn`t`ve\": \"must not have\",\n",
    "    \"needn`t\": \"need not\",\n",
    "    \"needn`t`ve\": \"need not have\",\n",
    "    \"o`clock\": \"of the clock\",\n",
    "    \"oughtn`t\": \"ought not\",\n",
    "    \"oughtn`t`ve\": \"ought not have\",\n",
    "    \"shan`t\": \"shall not\",\n",
    "    \"sha`n`t\": \"shall not\",\n",
    "    \"shan`t`ve\": \"shall not have\",\n",
    "    \"she`d\": \"she had / she would\",\n",
    "    \"she`d`ve\": \"she would have\",\n",
    "    \"she`ll\": \"she shall / she will\",\n",
    "    \"she`ll`ve\": \"she shall have / she will have\",\n",
    "    \"she`s\": \"she has / she is\",\n",
    "    \"should`ve\": \"should have\",\n",
    "    \"shouldn`t\": \"should not\",\n",
    "    \"shouldn`t`ve\": \"should not have\",\n",
    "    \"so`ve\": \"so have\",\n",
    "    \"so`s\": \"so as / so is\",\n",
    "    \"that`d\": \"that would / that had\",\n",
    "    \"that`d`ve\": \"that would have\",\n",
    "    \"that`s\": \"that has / that is\",\n",
    "    \"there`d\": \"there had / there would\",\n",
    "    \"there`d`ve\": \"there would have\",\n",
    "    \"there`s\": \"there has / there is\",\n",
    "    \"they`d\": \"they had / they would\",\n",
    "    \"they`d`ve\": \"they would have\",\n",
    "    \"they`ll\": \"they shall / they will\",\n",
    "    \"they`ll`ve\": \"they shall have / they will have\",\n",
    "    \"they`re\": \"they are\",\n",
    "    \"they`ve\": \"they have\",\n",
    "    \"to`ve\": \"to have\",\n",
    "    \"wasn`t\": \"was not\",\n",
    "    \"we`d\": \"we had / we would\",\n",
    "    \"we`d`ve\": \"we would have\",\n",
    "    \"we`ll\": \"we will\",\n",
    "    \"we`ll`ve\": \"we will have\",\n",
    "    \"we`re\": \"we are\",\n",
    "    \"we`ve\": \"we have\",\n",
    "    \"weren`t\": \"were not\",\n",
    "    \"what`ll\": \"what shall / what will\",\n",
    "    \"what`ll`ve\": \"what shall have / what will have\",\n",
    "    \"what`re\": \"what are\",\n",
    "    \"what`s\": \"what has / what is\",\n",
    "    \"what`ve\": \"what have\",\n",
    "    \"when`s\": \"when has / when is\",\n",
    "    \"when`ve\": \"when have\",\n",
    "    \"where`d\": \"where did\",\n",
    "    \"where`s\": \"where has / where is\",\n",
    "    \"where`ve\": \"where have\",\n",
    "    \"who`ll\": \"who shall / who will\",\n",
    "    \"who`ll`ve\": \"who shall have / who will have\",\n",
    "    \"who`s\": \"who has / who is\",\n",
    "    \"who`ve\": \"who have\",\n",
    "    \"why`s\": \"why has / why is\",\n",
    "    \"why`ve\": \"why have\",\n",
    "    \"will`ve\": \"will have\",\n",
    "    \"won`t\": \"will not\",\n",
    "    \"won`t`ve\": \"will not have\",\n",
    "    \"would`ve\": \"would have\",\n",
    "    \"wouldn`t\": \"would not\",\n",
    "    \"wouldn`t`ve\": \"would not have\",\n",
    "    \"y`all\": \"you all\",\n",
    "    \"y`all`d\": \"you all would\",\n",
    "    \"y`all`d`ve\": \"you all would have\",\n",
    "    \"y`all`re\": \"you all are\",\n",
    "    \"y`all`ve\": \"you all have\",\n",
    "    \"you`d\": \"you had / you would\",\n",
    "    \"you`d`ve\": \"you would have\",\n",
    "    \"you`ll\": \"you shall / you will\",\n",
    "    \"you`ll`ve\": \"you shall have / you will have\",\n",
    "    \"you`re\": \"you are\",\n",
    "    \"you`ve\": \"you have\"\n",
    "}\n",
    "\n",
    "extra_punctuations = ['', '.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', 'â€“', '&']\n",
    "stopword_list = stopwords.words('english') + list(string.punctuation) + extra_punctuations + ['u', 'the', 'us', 'say',\n",
    "                                                                                              'that', 'he', 'me', 'she',\n",
    "                                                                                              'get', 'rt', 'it', 'mt',\n",
    "                                                                                              'via', 'not', 'and',\n",
    "                                                                                              'let', 'so', 'say',\n",
    "                                                                                              'dont', 'use', 'you',\n",
    "                                                                                              'null']\n",
    "import regex as re\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def remove_punctuations(data):\n",
    "    punct_tag = re.compile(r'[^\\w\\s]')\n",
    "    data = punct_tag.sub(r'', data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def removeSpecialChars(data):\n",
    "    '''\n",
    "    Removes special characters which are specifically found in tweets.\n",
    "    '''\n",
    "    # Converts HTML tags to the characters they represent\n",
    "    # soup = BeautifulSoup(data, \"html.parser\")\n",
    "    # data = soup.get_text()\n",
    "\n",
    "    # Convert www.* or https?://* to empty strings\n",
    "    data = re.sub('((www\\.[^\\s]+)|(http?://[^\\s]+))', '', data)\n",
    "\n",
    "    # Convert @username to empty strings\n",
    "    data = re.sub('@[^\\s]+', '', data)\n",
    "\n",
    "    # remove org.apache. like texts\n",
    "    data = re.sub('(\\w+\\.){2,}', '', data)\n",
    "\n",
    "    # Remove additional white spaces\n",
    "    data = re.sub('[\\s]+', ' ', data)\n",
    "\n",
    "    data = re.sub('\\.(?!$)', '', data)\n",
    "\n",
    "    # Replace #word with word\n",
    "    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_nonenglish_charac(string):\n",
    "    return re.sub('[^a-zA-Z]', ' ', string)\n",
    "\n",
    "\n",
    "def text_cleaning(data):\n",
    "    \"\"\"Text Cleaning\n",
    "    let us clean the dataset and remove the redundancies.This includes\n",
    "\n",
    "    HTML codes\n",
    "    URLs\n",
    "    Emojis\n",
    "    Stopwords\n",
    "    Punctuations\n",
    "    Expanding Abbreviations\"\"\"\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        data = unidecode(data)\n",
    "    except Exception as ex:\n",
    "        print(f\"#### the data is : \" , data)\n",
    "        raise ex\n",
    "    data = expandContractions(data)\n",
    "    # tokens = word_tokenize(data)\n",
    "    # data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n",
    "    data = re.sub('\\b\\w{,2}\\b', '', data)\n",
    "    data = re.sub(' +', ' ', data)\n",
    "    data = removeSpecialChars(data)\n",
    "    data = remove_emoji(data)\n",
    "    # data = [stemmer.stem(w) for w in data.split()]\n",
    "    # data = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])\n",
    "    return data\n",
    "\n",
    "\n",
    "def step_1_get_dataframe(filename, filetype=\"json\"):\n",
    "    if filetype == \"csv\":\n",
    "        train_df = pd.read_csv(filename,encoding=encoding)\n",
    "    if filetype == \"json\":\n",
    "        train_df = pd.read_json(filename,encoding=encoding)\n",
    "    train_df.drop_duplicates(keep='first').count()\n",
    "    train_df = train_df.dropna()\n",
    "    train_df_shuffled = train_df.sample(frac=1, random_state=42)  # shuffle with random_state=42 for reproducibility\n",
    "    print(train_df_shuffled.head())\n",
    "    print(train_df_shuffled.columns)\n",
    "    return train_df_shuffled\n",
    "\n",
    "\n",
    "def remove_columns(df, cols: list, ):\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "    print(df.head)\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rename_column(df,original, new, ):\n",
    "    df.rename({original: new}, axis=1, inplace=True)\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def label_count_rebalancing(df,labels):\n",
    "    dataframes = []\n",
    "    print(\"### Old count: \")\n",
    "    print(df.labels.value_counts())\n",
    "    for label in labels:\n",
    "        dataframes.append(df[df['labels'] == label])\n",
    "\n",
    "    lowest = 9999999999\n",
    "    for df in dataframes:\n",
    "        if len(df) < lowest:\n",
    "            lowest = len(df)\n",
    "\n",
    "    balanced_df = []\n",
    "    for df in dataframes:\n",
    "        balanced_df.append(\n",
    "            resample(df, replace=False, n_samples=lowest)\n",
    "        )\n",
    "\n",
    "    train_df = pd.concat(balanced_df)\n",
    "    print(f\"### New counts \")\n",
    "    print(train_df.labels.value_counts())\n",
    "    return train_df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def unzip_data(filename):\n",
    "    \"\"\"\n",
    "  Unzips filename into the current working directory.\n",
    "\n",
    "  Args:\n",
    "    filename (str): a filepath to a target zip folder to be unzipped.\n",
    "  \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        zip_ref = zipfile.ZipFile(filename, \"r\")\n",
    "        zip_ref.extractall()\n",
    "        zip_ref.close()\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print(f\"File not found\")\n",
    "\n",
    "\n",
    "def apply_text_cleaning(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(lambda x: text_cleaning(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26f30d-032e-4aa0-ad48-0f2a39bb48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = step_1_get_dataframe(\"TheSocialDilemma.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
