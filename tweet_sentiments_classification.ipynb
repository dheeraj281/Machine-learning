{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX9jaseX77Bp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.utils import resample\n",
        "\n",
        "cList = {\n",
        "    \"i`m\": \"i am\",\n",
        "    \"you`re\": \"you are\",\n",
        "    \"it`s\": \"it is\",\n",
        "    \"we`re\": \"we are\",\n",
        "    \"we`ll\": \"we will\",\n",
        "    \"That`s\": \"that is\",\n",
        "    \"haven`t\": \"have not\",\n",
        "    \"let`s\": \"let us\",\n",
        "    \"ain`t\": \"am not / are not / is not / has not / have not\",\n",
        "    \"aren`t\": \"are not / am not\",\n",
        "    \"can`t\": \"cannot\",\n",
        "    \"can`t`ve\": \"cannot have\",\n",
        "    \"`cause\": \"because\",\n",
        "    \"could`ve\": \"could have\",\n",
        "    \"couldn`t\": \"could not\",\n",
        "    \"couldn`t`ve\": \"could not have\",\n",
        "    \"didn`t\": \"did not\",\n",
        "    \"doesn`t\": \"does not\",\n",
        "    \"don`t\": \"do not\",\n",
        "    \"hadn`t\": \"had not\",\n",
        "    \"hadn`t`ve\": \"had not have\",\n",
        "    \"hasn`t\": \"has not\",\n",
        "    \"haven`t\": \"have not\",\n",
        "    \"he`d\": \"he had / he would\",\n",
        "    \"he`d`ve\": \"he would have\",\n",
        "    \"he`ll\": \"he shall / he will\",\n",
        "    \"he`ll`ve\": \"he shall have / he will have\",\n",
        "    \"he`s\": \"he has / he is\",\n",
        "    \"how`d\": \"how did\",\n",
        "    \"how`d`y\": \"how do you\",\n",
        "    \"how`ll\": \"how will\",\n",
        "    \"how`s\": \"how has / how is / how does\",\n",
        "    \"I`d\": \"I had / I would\",\n",
        "    \"I`d`ve\": \"I would have\",\n",
        "    \"I`ll\": \"I shall / I will\",\n",
        "    \"I`ll`ve\": \"I shall have / I will have\",\n",
        "    \"I`m\": \"I am\",\n",
        "    \"I`ve\": \"I have\",\n",
        "    \"isn`t\": \"is not\",\n",
        "    \"it`d\": \"it had / it would\",\n",
        "    \"it`d`ve\": \"it would have\",\n",
        "    \"it`ll\": \"it shall / it will\",\n",
        "    \"it`ll`ve\": \"it shall have / it will have\",\n",
        "    \"it`s\": \"it has / it is\",\n",
        "    \"let`s\": \"let us\",\n",
        "    \"ma`am\": \"madam\",\n",
        "    \"mayn`t\": \"may not\",\n",
        "    \"might`ve\": \"might have\",\n",
        "    \"mightn`t\": \"might not\",\n",
        "    \"mightn`t`ve\": \"might not have\",\n",
        "    \"must`ve\": \"must have\",\n",
        "    \"mustn`t\": \"must not\",\n",
        "    \"mustn`t`ve\": \"must not have\",\n",
        "    \"needn`t\": \"need not\",\n",
        "    \"needn`t`ve\": \"need not have\",\n",
        "    \"o`clock\": \"of the clock\",\n",
        "    \"oughtn`t\": \"ought not\",\n",
        "    \"oughtn`t`ve\": \"ought not have\",\n",
        "    \"shan`t\": \"shall not\",\n",
        "    \"sha`n`t\": \"shall not\",\n",
        "    \"shan`t`ve\": \"shall not have\",\n",
        "    \"she`d\": \"she had / she would\",\n",
        "    \"she`d`ve\": \"she would have\",\n",
        "    \"she`ll\": \"she shall / she will\",\n",
        "    \"she`ll`ve\": \"she shall have / she will have\",\n",
        "    \"she`s\": \"she has / she is\",\n",
        "    \"should`ve\": \"should have\",\n",
        "    \"shouldn`t\": \"should not\",\n",
        "    \"shouldn`t`ve\": \"should not have\",\n",
        "    \"so`ve\": \"so have\",\n",
        "    \"so`s\": \"so as / so is\",\n",
        "    \"that`d\": \"that would / that had\",\n",
        "    \"that`d`ve\": \"that would have\",\n",
        "    \"that`s\": \"that has / that is\",\n",
        "    \"there`d\": \"there had / there would\",\n",
        "    \"there`d`ve\": \"there would have\",\n",
        "    \"there`s\": \"there has / there is\",\n",
        "    \"they`d\": \"they had / they would\",\n",
        "    \"they`d`ve\": \"they would have\",\n",
        "    \"they`ll\": \"they shall / they will\",\n",
        "    \"they`ll`ve\": \"they shall have / they will have\",\n",
        "    \"they`re\": \"they are\",\n",
        "    \"they`ve\": \"they have\",\n",
        "    \"to`ve\": \"to have\",\n",
        "    \"wasn`t\": \"was not\",\n",
        "    \"we`d\": \"we had / we would\",\n",
        "    \"we`d`ve\": \"we would have\",\n",
        "    \"we`ll\": \"we will\",\n",
        "    \"we`ll`ve\": \"we will have\",\n",
        "    \"we`re\": \"we are\",\n",
        "    \"we`ve\": \"we have\",\n",
        "    \"weren`t\": \"were not\",\n",
        "    \"what`ll\": \"what shall / what will\",\n",
        "    \"what`ll`ve\": \"what shall have / what will have\",\n",
        "    \"what`re\": \"what are\",\n",
        "    \"what`s\": \"what has / what is\",\n",
        "    \"what`ve\": \"what have\",\n",
        "    \"when`s\": \"when has / when is\",\n",
        "    \"when`ve\": \"when have\",\n",
        "    \"where`d\": \"where did\",\n",
        "    \"where`s\": \"where has / where is\",\n",
        "    \"where`ve\": \"where have\",\n",
        "    \"who`ll\": \"who shall / who will\",\n",
        "    \"who`ll`ve\": \"who shall have / who will have\",\n",
        "    \"who`s\": \"who has / who is\",\n",
        "    \"who`ve\": \"who have\",\n",
        "    \"why`s\": \"why has / why is\",\n",
        "    \"why`ve\": \"why have\",\n",
        "    \"will`ve\": \"will have\",\n",
        "    \"won`t\": \"will not\",\n",
        "    \"won`t`ve\": \"will not have\",\n",
        "    \"would`ve\": \"would have\",\n",
        "    \"wouldn`t\": \"would not\",\n",
        "    \"wouldn`t`ve\": \"would not have\",\n",
        "    \"y`all\": \"you all\",\n",
        "    \"y`all`d\": \"you all would\",\n",
        "    \"y`all`d`ve\": \"you all would have\",\n",
        "    \"y`all`re\": \"you all are\",\n",
        "    \"y`all`ve\": \"you all have\",\n",
        "    \"you`d\": \"you had / you would\",\n",
        "    \"you`d`ve\": \"you would have\",\n",
        "    \"you`ll\": \"you shall / you will\",\n",
        "    \"you`ll`ve\": \"you shall have / you will have\",\n",
        "    \"you`re\": \"you are\",\n",
        "    \"you`ve\": \"you have\"\n",
        "}\n",
        "\n",
        "extra_punctuations = ['', '.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', 'â€“', '&']\n",
        "stopword_list = stopwords.words('english') + list(string.punctuation) + extra_punctuations + ['u', 'the', 'us', 'say',\n",
        "                                                                                              'that', 'he', 'me', 'she',\n",
        "                                                                                              'get', 'rt', 'it', 'mt',\n",
        "                                                                                              'via', 'not', 'and',\n",
        "                                                                                              'let', 'so', 'say',\n",
        "                                                                                              'dont', 'use', 'you',\n",
        "                                                                                              'null']\n",
        "import regex as re\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "\n",
        "def remove_punctuations(data):\n",
        "    punct_tag = re.compile(r'[^\\w\\s]')\n",
        "    data = punct_tag.sub(r'', data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def removeSpecialChars(data):\n",
        "    '''\n",
        "    Removes special characters which are specifically found in tweets.\n",
        "    '''\n",
        "    # Converts HTML tags to the characters they represent\n",
        "    # soup = BeautifulSoup(data, \"html.parser\")\n",
        "    # data = soup.get_text()\n",
        "\n",
        "    # Convert www.* or https?://* to empty strings\n",
        "    data = re.sub('((www\\.[^\\s]+)|(http?://[^\\s]+))', '', data)\n",
        "\n",
        "    # Convert @username to empty strings\n",
        "    data = re.sub('@[^\\s]+', '', data)\n",
        "\n",
        "    # remove org.apache. like texts\n",
        "    data = re.sub('(\\w+\\.){2,}', '', data)\n",
        "\n",
        "    # Remove additional white spaces\n",
        "    data = re.sub('[\\s]+', ' ', data)\n",
        "\n",
        "    data = re.sub('\\.(?!$)', '', data)\n",
        "\n",
        "    # Replace #word with word\n",
        "    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def remove_nonenglish_charac(string):\n",
        "    return re.sub('[^a-zA-Z]', ' ', string)\n",
        "\n",
        "\n",
        "def text_cleaning(data):\n",
        "    \"\"\"Text Cleaning\n",
        "    let us clean the dataset and remove the redundancies.This includes\n",
        "\n",
        "    HTML codes\n",
        "    URLs\n",
        "    Emojis\n",
        "    Stopwords\n",
        "    Punctuations\n",
        "    Expanding Abbreviations\"\"\"\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    try:\n",
        "        data = unidecode(data)\n",
        "    except Exception as ex:\n",
        "        print(f\"#### the data is : \" , data)\n",
        "        raise ex\n",
        "    data = expandContractions(data)\n",
        "    # tokens = word_tokenize(data)\n",
        "    # data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n",
        "    data = re.sub('\\b\\w{,2}\\b', '', data)\n",
        "    data = re.sub(' +', ' ', data)\n",
        "    data = removeSpecialChars(data)\n",
        "    data = remove_emoji(data)\n",
        "    # data = [stemmer.stem(w) for w in data.split()]\n",
        "    # data = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])\n",
        "    return data\n",
        "\n",
        "\n",
        "def step_1_get_dataframe(filename, filetype=\"json\"):\n",
        "    if filetype == \"csv\":\n",
        "        train_df = pd.read_csv(filename)\n",
        "    if filetype == \"json\":\n",
        "        train_df = pd.read_json(filename)\n",
        "    train_df.drop_duplicates(keep='first').count()\n",
        "    train_df = train_df.dropna()\n",
        "    train_df_shuffled = train_df.sample(frac=1, random_state=42)  # shuffle with random_state=42 for reproducibility\n",
        "    print(train_df_shuffled.head())\n",
        "    print(train_df_shuffled.columns)\n",
        "    return train_df_shuffled\n",
        "\n",
        "\n",
        "def remove_columns(df, cols: list, ):\n",
        "    df.drop(cols, axis=1, inplace=True)\n",
        "    print(df.head)\n",
        "    print(df.columns)\n",
        "    return df\n",
        "\n",
        "\n",
        "def rename_column(df,original, new, ):\n",
        "    df.rename({original: new}, axis=1, inplace=True)\n",
        "    print(df.columns)\n",
        "    return df\n",
        "\n",
        "\n",
        "def label_count_rebalancing(df,labels):\n",
        "    dataframes = []\n",
        "    print(\"### Old count: \")\n",
        "    print(df.labels.value_counts())\n",
        "    for label in labels:\n",
        "        dataframes.append(df[df['labels'] == label])\n",
        "\n",
        "    lowest = 9999999999\n",
        "    for df in dataframes:\n",
        "        if len(df) < lowest:\n",
        "            lowest = len(df)\n",
        "\n",
        "    balanced_df = []\n",
        "    for df in dataframes:\n",
        "        balanced_df.append(\n",
        "            resample(df, replace=False, n_samples=lowest)\n",
        "        )\n",
        "\n",
        "    train_df = pd.concat(balanced_df)\n",
        "    print(f\"### New counts \")\n",
        "    print(train_df.labels.value_counts())\n",
        "    return train_df.sample(frac=1, random_state=42)\n",
        "\n",
        "\n",
        "def unzip_data(filename):\n",
        "    \"\"\"\n",
        "  Unzips filename into the current working directory.\n",
        "\n",
        "  Args:\n",
        "    filename (str): a filepath to a target zip folder to be unzipped.\n",
        "  \"\"\"\n",
        "    if os.path.exists(filename):\n",
        "        zip_ref = zipfile.ZipFile(filename, \"r\")\n",
        "        zip_ref.extractall()\n",
        "        zip_ref.close()\n",
        "        os.remove(filename)\n",
        "    else:\n",
        "        print(f\"File not found\")\n",
        "\n",
        "\n",
        "def apply_text_cleaning(df, column_name):\n",
        "    df[column_name] = df[column_name].apply(lambda x: text_cleaning(x))\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import layers\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "eBb-vUH-8r5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = step_1_get_dataframe(filename=\"Tweets.csv\", filetype=\"csv\")\n",
        "\n",
        "df = remove_columns(cols=['textID', 'selected_text'], df=df)\n",
        "\n",
        "df = rename_column(original=\"sentiment\", new=\"labels\", df=df)\n",
        "\n",
        "# df = label_count_rebalancing(labels=['neutral', 'positive', 'negative'], df=df)\n",
        "\n",
        "train_df = apply_text_cleaning(df=df, column_name=\"text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "XEx8rBsg8DKA",
        "outputId": "c15983f7-f8a7-4fa0-88fd-c9365f43aac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3d43b47b96c9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_1_get_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tweets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrename_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d8397a2c02d0>\u001b[0m in \u001b[0;36mstep_1_get_dataframe\u001b[0;34m(filename, filetype)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstep_1_get_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tweets.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rem_ind = []\n",
        "for ind in train_df.index:\n",
        "    tweet = train_df['text'][ind]\n",
        "    if len(tweet.split()) > 10:\n",
        "        rem_ind.append(ind)"
      ],
      "metadata": {
        "id": "LzjBbn1u9Dhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(rem_ind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tmkuSrbv9i3r",
        "outputId": "15f33a09-6723-4a2f-dd11-f75e454cb916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           text    labels\n",
              "1589            Enjoy! Family trumps everything  positive\n",
              "6562   Clive it has / it is my birthday pat me    neutral\n",
              "2603                               congrats hey  positive\n",
              "4004                                 is texting   neutral\n",
              "27232                           Tell him where.   neutral\n",
              "...                                         ...       ...\n",
              "11285     i wish paramore would come to ireland   neutral\n",
              "21576                    feels like warm things   neutral\n",
              "5391      My best friend is in vegas without me   neutral\n",
              "861          - fire and urban at rock challenge   neutral\n",
              "15796                     A+ for effort though   positive\n",
              "\n",
              "[11472 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0be0d8ea-3f50-4341-a70d-12fb5499b0c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1589</th>\n",
              "      <td>Enjoy! Family trumps everything</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6562</th>\n",
              "      <td>Clive it has / it is my birthday pat me</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2603</th>\n",
              "      <td>congrats hey</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4004</th>\n",
              "      <td>is texting</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27232</th>\n",
              "      <td>Tell him where.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11285</th>\n",
              "      <td>i wish paramore would come to ireland</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21576</th>\n",
              "      <td>feels like warm things</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5391</th>\n",
              "      <td>My best friend is in vegas without me</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>861</th>\n",
              "      <td>- fire and urban at rock challenge</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15796</th>\n",
              "      <td>A+ for effort though</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11472 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0be0d8ea-3f50-4341-a70d-12fb5499b0c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0be0d8ea-3f50-4341-a70d-12fb5499b0c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0be0d8ea-3f50-4341-a70d-12fb5499b0c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce4678e3-0c6b-46a1-830e-185d1088dba0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce4678e3-0c6b-46a1-830e-185d1088dba0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce4678e3-0c6b-46a1-830e-185d1088dba0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df['text'].values\n",
        "y = train_df['labels'].values\n",
        "\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "y = to_categorical(y)"
      ],
      "metadata": {
        "id": "719Hdyeo9lOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5], y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psP8hcjE-Fuc",
        "outputId": "a7d03ddc-83dc-468a-b945-4fe1c9cdb22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([' Enjoy! Family trumps everything',\n",
              "        ' --of them kinda turns me off of it all And then I buy more of them and dig a deeper hole, etc ;;',\n",
              "        'Clive it has / it is my birthday pat me ', ' congrats hey',\n",
              "        'is texting'], dtype=object),\n",
              " array([[0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(X)"
      ],
      "metadata": {
        "id": "Xw97Z1XP-Hxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGZk4OEo-LEc",
        "outputId": "8a6e1f66-4b2e-45b2-9c62-9668124ccca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27075"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "JHFioNH9-Nl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seq = tokenizer.texts_to_sequences(train_text)\n",
        "val_seq = tokenizer.texts_to_sequences(val_text)"
      ],
      "metadata": {
        "id": "7q6JKuo9-P8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pad = tf.keras.preprocessing.sequence.pad_sequences(train_seq,maxlen=16,padding=\"post\",truncating=\"post\")\n",
        "val_pad = tf.keras.preprocessing.sequence.pad_sequences(val_seq,maxlen=16,padding=\"post\",truncating=\"post\")"
      ],
      "metadata": {
        "id": "2hiQ29Rw-SwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading twitter glove embeddign words.\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqAPGDxS-ViU",
        "outputId": "14b32b27-6fce-40c5-e3d5-f5043cc840d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-28 04:08:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-09-28 04:08:52--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-09-28 04:11:31 (5.19 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"glove.6B.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall()"
      ],
      "metadata": {
        "id": "sIjBCRGw_JwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to file containing the embeddings\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "\n",
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs\n",
        "\n",
        "test_word = 'everything'\n",
        "\n",
        "test_vector = GLOVE_EMBEDDINGS[test_word]\n",
        "\n",
        "print(f\"Vector representation of word {test_word} looks like this:\\n\\n{test_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avJe5bRw-aPB",
        "outputId": "07982544-e156-4b77-b85a-95d4968f4a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of word everything looks like this:\n",
            "\n",
            "[ 0.013026   0.33335    0.62812   -0.089324  -0.13434    0.26948\n",
            " -0.17045    0.039592   0.19847   -0.054669   0.59498    0.31745\n",
            "  0.28691    0.42015   -0.23682   -0.3583    -0.45421    0.87357\n",
            " -0.26887    0.38228   -0.013516   0.18781    0.11409   -0.91682\n",
            "  0.17019    0.14309   -0.52976   -0.9702    -0.21943   -0.4512\n",
            " -0.25972    0.55875   -0.07939    0.098255  -0.15482    0.11926\n",
            "  0.034281  -0.079117  -0.18668   -0.64334    0.026627  -0.15963\n",
            "  0.12529   -0.44979   -0.99793    0.13604   -0.28778   -0.059987\n",
            "  0.019177  -1.2517     0.5363     0.37451    0.12018    0.93167\n",
            " -0.16836   -1.9662     0.19831    0.70928    1.3088     0.22569\n",
            " -0.028412   1.2327    -0.44345   -0.34264    0.6256     0.43331\n",
            "  0.95851    0.030527  -0.10855   -0.17556    0.37907   -0.12585\n",
            "  0.32332    0.10456    0.80106    0.32022   -0.073298   0.020626\n",
            " -0.53155    0.47124    0.73076    0.0048294 -0.60191    0.36261\n",
            " -1.2432    -0.070735  -0.22735   -0.16984   -0.77984   -0.37359\n",
            "  0.15763   -0.20682    0.17747   -0.16221   -0.60065   -0.62195\n",
            " -0.087764   0.10573    0.41712    0.80295  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Initialize an empty numpy array with the appropriate size\n",
        "EMBEDDINGS_MATRIX = np.zeros((vocab_size+1, EMBEDDING_DIM))\n",
        "\n",
        "# Iterate all of the words in the vocabulary and if the vector representation for\n",
        "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        EMBEDDINGS_MATRIX[i] = embedding_vector"
      ],
      "metadata": {
        "id": "wpDNxjiO-qvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.Embedding(input_dim=vocab_size+1,\n",
        "                                      output_dim=EMBEDDING_DIM,\n",
        "                                      input_length=16,\n",
        "                                      weights=[EMBEDDINGS_MATRIX]),\n",
        "            tf.keras.layers.LSTM(64,return_sequences=True,dropout=0.3),\n",
        "            tf.keras.layers.LSTM(32,dropout=0.5),\n",
        "            tf.keras.layers.Dense(16,activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(3,activation=\"softmax\")\n",
        "\n",
        "        ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy']\n",
        "\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axUMFQmG-sCo",
        "outputId": "bee187ab-e95e-42da-bbd2-e18b7b1bde0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 16, 100)           2707600   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16, 64)            42240     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2762835 (10.54 MB)\n",
            "Trainable params: 2762835 (10.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_pad,train_labels,epochs=2,validation_data=(val_pad,val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Hp8p3p-0Qg",
        "outputId": "5fcb2797-0895-46c8-ee4d-af21c7bf12a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "645/645 [==============================] - 58s 90ms/step - loss: 0.3271 - accuracy: 0.8735 - val_loss: 0.8652 - val_accuracy: 0.6933\n",
            "Epoch 2/2\n",
            "645/645 [==============================] - 50s 77ms/step - loss: 0.2910 - accuracy: 0.8903 - val_loss: 0.9243 - val_accuracy: 0.6879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nA_B1J06J7W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size + 1,\n",
        "            output_dim=EMBEDDING_DIM,\n",
        "            input_length=16,\n",
        "            weights=[EMBEDDINGS_MATRIX],\n",
        "            trainable=False  # Fix embedding layer weights\n",
        "        ),\n",
        "        tf.keras.layers.LSTM(32, return_sequences=True, dropout=0.2),\n",
        "        tf.keras.layers.LSTM(16, dropout=0.2),\n",
        "        tf.keras.layers.Dense(8, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        tf.keras.layers.Dropout(0.5),  # Adding dropout to dense layer\n",
        "        tf.keras.layers.Dense(3, activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,  # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_pad,\n",
        "    train_labels,\n",
        "    epochs=50,\n",
        "    validation_data=(val_pad, val_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "F8Gar6TRIXB_",
        "outputId": "9336005f-e589-428a-e02a-a08b7abfb9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b64f10fc712d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = tf.keras.models.Sequential(\n\u001b[0m\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         tf.keras.layers.Embedding(\n\u001b[1;32m      4\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_pad,val_labels)"
      ],
      "metadata": {
        "id": "QBcvAcWW-0vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = [*range(10)]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vW2ANTRc-3-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import linregress\n",
        "\n",
        "slope, *_ = linregress(epochs, val_loss)\n",
        "print(f\"The slope of your validation loss curve is {slope:.5f}\") # should be lesse than 0.0005"
      ],
      "metadata": {
        "id": "ISl1RPyQ-9GV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}